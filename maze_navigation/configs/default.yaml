# Default configuration for maze navigation RL experiments

# Algorithm selection
algorithm: dqn

# Random seed for reproducibility
seed: 42

# Compute device: 'cpu', 'cuda', or 'auto'
device: auto

# Experiment naming
experiment_name: null
log_dir: experiments

# Environment configuration
env:
  rows: 20
  cols: 20
  stochasticity: 0.02
  max_episode_steps: 1000

# Reward structure
rewards:
  goal: 200.0
  oil: -5.0
  bump: -10.0
  action: -1.0

# Training configuration
training:
  total_timesteps: 100000
  eval_freq: 5000
  save_freq: 10000
  log_freq: 100
  n_eval_episodes: 10

# Visualization settings
visualization:
  enabled: false
  save_only: true
  save_path: figures

# DQN configuration (used for dqn, ddqn, dueling_dqn)
dqn:
  hidden_dims: [256, 256]
  learning_rate: 0.0001
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay_steps: 10000
  target_update_freq: 1000
  tau: 1.0
  buffer_size: 100000
  batch_size: 64
  min_buffer_size: 1000

# PPO configuration
ppo:
  hidden_dims: [256, 256]
  learning_rate: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  n_steps: 2048
  n_epochs: 10
  batch_size: 64
