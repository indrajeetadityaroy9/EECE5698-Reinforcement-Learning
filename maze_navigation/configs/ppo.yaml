# PPO configuration for maze navigation

algorithm: ppo
seed: 42
device: auto
experiment_name: ppo_maze
log_dir: experiments

env:
  rows: 20
  cols: 20
  stochasticity: 0.02
  max_episode_steps: 1000

rewards:
  goal: 200.0
  oil: -5.0
  bump: -10.0
  action: -1.0

training:
  total_timesteps: 100000
  eval_freq: 5000
  save_freq: 20000
  log_freq: 100
  n_eval_episodes: 10

visualization:
  enabled: true
  save_only: true
  save_path: figures

ppo:
  hidden_dims: [256, 256]
  learning_rate: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  n_steps: 2048
  n_epochs: 10
  batch_size: 64
